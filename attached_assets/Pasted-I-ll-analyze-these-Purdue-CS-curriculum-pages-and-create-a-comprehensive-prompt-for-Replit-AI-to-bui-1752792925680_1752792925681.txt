I'll analyze these Purdue CS curriculum pages and create a comprehensive prompt for Replit AI to build an efficient scraper with N8N integration.Based on my analysis of the Purdue CS curriculum pages, here's a comprehensive prompt for Replit AI to build an efficient web scraper with N8N integration:

## **Comprehensive Prompt for Replit AI: Purdue CS Curriculum Scraper with N8N Integration**

### **Project Overview**
Build a sophisticated web scraper for Purdue Computer Science curriculum data with N8N workflow automation. The system should extract, process, and store curriculum requirements focusing on the Machine Intelligence and Software Engineering tracks - the two most in-demand specializations at Purdue CS.

### **Technical Architecture Requirements**

#### **1. Core Scraping Engine**
```javascript
// Implement using Puppeteer for dynamic content handling
const puppeteer = require('puppeteer');
const cheerio = require('cheerio');
const fs = require('fs').promises;

class PurdueCScraper {
  constructor() {
    this.baseUrls = {
      main: 'https://www.cs.purdue.edu/undergraduate/curriculum/bachelor.html',
      machineIntelligence: 'https://www.cs.purdue.edu/undergraduate/curriculum/track-mI-fall2023.html',
      softwareEngineering: 'https://www.cs.purdue.edu/undergraduate/curriculum/track-softengr-fall2023.html'
    };
    this.browser = null;
  }

  async initialize() {
    this.browser = await puppeteer.launch({ 
      headless: 'new',
      args: ['--no-sandbox', '--disable-setuid-sandbox']
    });
  }
}
```

#### **2. Data Structure Design**
```javascript
// Define TypeScript interfaces for type safety
interface Course {
  code: string;
  title: string;
  credits: number;
  prerequisites: string[];
  description?: string;
  url: string;
}

interface Track {
  name: string;
  objectives: string;
  requiredCourses: Course[];
  electiveCourses: Course[];
  additionalRequirements: string[];
  trackChair: string;
}

interface Curriculum {
  coreCourses: Course[];
  tracks: {
    machineIntelligence: Track;
    softwareEngineering: Track;
  };
  generalRequirements: string[];
  lastUpdated: Date;
}
```

#### **3. Scraping Methods Implementation**
```javascript
async scrapeCourseDetails(courseUrl) {
  // Extract from course catalog URLs like:
  // https://selfservice.mypurdue.purdue.edu/prod/bzwsrch.p_catalog_detail?subject=CS&term=CURRENT&cnbr=18200
  
  const page = await this.browser.newPage();
  await page.goto(courseUrl, { waitUntil: 'networkidle2' });
  
  const courseData = await page.evaluate(() => {
    // DOM extraction logic
    return {
      title: document.querySelector('.nttitle')?.textContent,
      credits: document.querySelector('.credit_hours')?.textContent,
      description: document.querySelector('.courseblockdesc')?.textContent,
      prerequisites: Array.from(document.querySelectorAll('.prereq')).map(el => el.textContent)
    };
  });
  
  return courseData;
}

async scrapeTrackRequirements(trackUrl) {
  // Parse track-specific requirements
  const response = await fetch(trackUrl);
  const html = await response.text();
  const $ = cheerio.load(html);
  
  const requirements = {
    required: [],
    electives: []
  };
  
  // Extract required courses from tables
  $('table').each((i, table) => {
    const headers = $(table).find('th').text();
    if (headers.includes('Required')) {
      $(table).find('tr').each((j, row) => {
        const courseLink = $(row).find('a');
        if (courseLink.length) {
          requirements.required.push({
            code: courseLink.text(),
            url: courseLink.attr('href')
          });
        }
      });
    }
  });
  
  return requirements;
}
```

#### **4. N8N Workflow Integration**
```yaml
# n8n-workflow-config.json
{
  "name": "Purdue CS Curriculum Pipeline",
  "nodes": [
    {
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.cron",
      "parameters": {
        "cronExpression": "0 0 * * 0" # Weekly on Sundays
      }
    },
    {
      "name": "Execute Scraper",
      "type": "n8n-nodes-base.executeCommand",
      "parameters": {
        "command": "node scraper/index.js"
      }
    },
    {
      "name": "Transform Data",
      "type": "n8n-nodes-base.function",
      "parameters": {
        "functionCode": `
          const scrapedData = items[0].json;
          
          // Transform for different outputs
          return {
            courses: transformCourses(scrapedData),
            tracks: transformTracks(scrapedData),
            prerequisites: buildPrerequisiteGraph(scrapedData)
          };
        `
      }
    },
    {
      "name": "Store in Database",
      "type": "n8n-nodes-base.postgres",
      "parameters": {
        "operation": "upsert",
        "table": "cs_curriculum",
        "columns": "code,title,track,requirements"
      }
    },
    {
      "name": "Generate API Response",
      "type": "n8n-nodes-base.webhook",
      "parameters": {
        "path": "cs-curriculum",
        "responseMode": "lastNode"
      }
    }
  ]
}
```

#### **5. Data Processing Pipeline**
```javascript
class DataProcessor {
  constructor() {
    this.courseCache = new Map();
  }

  async processScrapedData(rawData) {
    // 1. Normalize course codes (CS 18200 -> CS18200)
    // 2. Build prerequisite graph
    // 3. Extract credit hours
    // 4. Link course descriptions
    
    const processed = {
      timestamp: new Date().toISOString(),
      coreCourses: await this.processCoreCourses(rawData.core),
      tracks: {
        machineIntelligence: await this.processTrack(rawData.mi),
        softwareEngineering: await this.processTrack(rawData.se)
      }
    };
    
    return processed;
  }

  buildPrerequisiteGraph(courses) {
    // Create directed graph of prerequisites
    const graph = new Map();
    
    courses.forEach(course => {
      graph.set(course.code, {
        ...course,
        prerequisites: course.prerequisites || [],
        dependents: []
      });
    });
    
    // Build reverse dependencies
    graph.forEach((course, code) => {
      course.prerequisites.forEach(prereq => {
        if (graph.has(prereq)) {
          graph.get(prereq).dependents.push(code);
        }
      });
    });
    
    return graph;
  }
}
```

#### **6. API Endpoints Structure**
```javascript
// Express.js API setup
const express = require('express');
const app = express();

// Endpoints
app.get('/api/curriculum/core', getCoreRequirements);
app.get('/api/curriculum/tracks/:trackName', getTrackRequirements);
app.get('/api/curriculum/course/:courseCode', getCourseDetails);
app.get('/api/curriculum/prerequisites/:courseCode', getPrerequisiteChain);
app.get('/api/curriculum/roadmap/:track', generateRoadmap);

// Advanced queries
app.post('/api/curriculum/validate-schedule', validateStudentSchedule);
app.get('/api/curriculum/electives/:track', getElectiveRecommendations);
```

#### **7. Database Schema**
```sql
-- PostgreSQL schema
CREATE TABLE courses (
  code VARCHAR(10) PRIMARY KEY,
  title VARCHAR(200) NOT NULL,
  credits INTEGER,
  description TEXT,
  last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tracks (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100) NOT NULL,
  objectives TEXT,
  track_chair VARCHAR(100),
  active BOOLEAN DEFAULT true
);

CREATE TABLE track_requirements (
  track_id INTEGER REFERENCES tracks(id),
  course_code VARCHAR(10) REFERENCES courses(code),
  requirement_type ENUM('required', 'elective'),
  notes TEXT,
  PRIMARY KEY (track_id, course_code)
);

CREATE TABLE prerequisites (
  course_code VARCHAR(10) REFERENCES courses(code),
  prerequisite_code VARCHAR(10) REFERENCES courses(code),
  requirement_type ENUM('mandatory', 'one_of', 'concurrent'),
  PRIMARY KEY (course_code, prerequisite_code)
);
```

#### **8. Error Handling & Monitoring**
```javascript
class ScraperMonitor {
  constructor() {
    this.errors = [];
    this.metrics = {
      lastRun: null,
      coursesScraped: 0,
      failedRequests: 0
    };
  }

  async wrapScraping(fn, context) {
    try {
      const result = await fn();
      this.metrics.coursesScraped++;
      return result;
    } catch (error) {
      this.errors.push({
        timestamp: new Date(),
        context,
        error: error.message,
        stack: error.stack
      });
      this.metrics.failedRequests++;
      
      // Send to N8N error handler
      await this.notifyN8N(error);
    }
  }
}
```

#### **9. N8N Knowledge Tree Structure**
```json
{
  "knowledge_tree": {
    "root": "Purdue CS Curriculum",
    "branches": {
      "core_requirements": {
        "courses": ["CS18200", "CS24000", "CS25000", "CS25100", "CS25200"],
        "metadata": {
          "total_credits": 15,
          "typical_completion": "4 semesters"
        }
      },
      "tracks": {
        "machine_intelligence": {
          "required": ["CS38100/CS47300", "CS37300", "STAT41600/MA41600/STAT51200"],
          "electives": {
            "min_required": 3,
            "options": ["CS34800", "CS35200", "CS44800", "CS45600", "CS45800"]
          }
        },
        "software_engineering": {
          "required": ["CS35200/CS35400", "CS35400", "CS38100", "CS40800"],
          "electives": {
            "min_required": 2,
            "options": ["CS35100", "CS35300", "CS37300", "CS42200", "CS42600"]
          }
        }
      }
    }
  }
}
```

#### **10. CLI Integration**
```javascript
// cli.js
const { program } = require('commander');
const scraper = require('./scraper');

program
  .command('scrape [target]')
  .description('Scrape Purdue CS curriculum data')
  .option('-t, --track <track>', 'specific track to scrape')
  .option('-o, --output <format>', 'output format (json|csv|sql)', 'json')
  .action(async (target, options) => {
    const data = await scraper.run({
      target: target || 'all',
      track: options.track,
      format: options.output
    });
    
    console.log(`Scraped ${data.courses.length} courses`);
  });

program
  .command('sync')
  .description('Sync with N8N workflow')
  .action(async () => {
    await scraper.syncWithN8N();
  });

program.parse();
```

### **Implementation Steps for Replit**

1. **Initialize Project Structure**:
```bash
mkdir purdue-cs-scraper
cd purdue-cs-scraper
npm init -y
npm install puppeteer cheerio express pg n8n-core commander
npm install -D typescript @types/node nodemon
```

2. **Set up N8N Docker Container**:
```yaml
# docker-compose.yml
version: '3.8'
services:
  n8n:
    image: n8nio/n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=admin
    volumes:
      - ./n8n-data:/home/node/.n8n
  
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: purdue_cs
      POSTGRES_PASSWORD: password
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
```

3. **Environment Configuration**:
```bash
# .env
DATABASE_URL=postgresql://user:password@localhost:5432/purdue_cs
N8N_WEBHOOK_URL=http://localhost:5678/webhook/purdue-cs
SCRAPER_SCHEDULE=0 0 * * 0
CACHE_TTL=86400
```

4. **Run the Complete Pipeline**:
```bash
# Start services
docker-compose up -d

# Run initial scrape
npm run scrape:all

# Start API server
npm run server

# Monitor N8N workflows
open http://localhost:5678
```

This architecture provides:
- Robust scraping with error recovery
- Efficient caching and incremental updates
- N8N workflow automation for scheduling and processing
- RESTful API for curriculum data access
- Prerequisite graph analysis
- Track comparison tools
- Student schedule validation

The system is designed to handle Purdue's complex curriculum structure while being maintainable and extensible for future track additions.